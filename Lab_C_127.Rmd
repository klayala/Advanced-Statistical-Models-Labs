---
title: "PSTAT 127 Homework 5"
author: "Kevin Ayala"
date: "3/6/2019"
output: pdf_document
---


(a) Load the faraway package and take a look at the data description by typing ?fat into
the R console. Do you suspect that some regularization may be helpful in fitting this
linear model? Explain.

No I do not think that regularization will be somewhat helpful here, we note that there are around 200 observations,
thus we do not have to worry about such things such as curse of dimensionality here. Since there are 18 variables (and not like 500 or a million), we can eliminate some by normal means such as finding significant predictors and so on from regular regression. 
```{r}
library(faraway)
?fat
head(fat)
```

b) Use the code below to divide your data into two sets - a training set fatTrain used to
fit the model and a testing set fatTest.

```{r}
set.seed(123) # ensures that everyone uses the same data split
# will give you a testing data set of 25 men, training set of 227

# Create matrix version of input

test.ind <- sample.int(n = nrow(fat), size = floor(0.1*nrow(fat))) #is the same as exameple 
train.ind <- setdiff(1:nrow(fat), test.ind) #is the same as the example 
fatTrain <- fat[train.ind,]
fatTest <- fat[test.ind,]
```

c)
Using the training data fatTrain, fit the linear model using four methods
i) Ordinary Least Squares with all predictors
ii) Ordinary Least Squares after performing backward stepwise selection using AIC
iii) Ridge regression, using $\lambda$ = 0.5
iv) Lasso regression, using $\lambda$ = 0.1


```{r}
library(glmnet)

linear_model <- lm(siri~.-brozek-density, data = fatTrain)
summary(linear_model)

AIC_lin_reg <- step(linear_model, direction = 'backward')
summary(AIC_lin_reg)

X_train <- as.matrix(fatTrain[4:18]) # removing density and brozek,
Y_train <- fatTrain$siri

X_test <-as.matrix(fatTest[4:18])
Y_test <- fatTest$siri

ridge_model <- glmnet(x=X_train, y=Y_train, alpha = 0, lambda = .5)

lasso_model <- glmnet(x=X_train, y=Y_train, alpha = 1, lambda = .1)

```

Use the fitted models to compute predicted body fat percentages for the test data. Which
method has the lowest average squared prediction error on the testing data?

```{r}
linear_model_pred <- predict(linear_model, newdata = fatTest)
linear_model_error <- sum((Y_test - linear_model_pred)^2)
linear_model_error

AIC_lin_reg_pred <- predict(AIC_lin_reg, newdata = fatTest)
AIC_model_error <- sum((Y_test - AIC_lin_reg_pred)^2)
AIC_model_error

ridge_predict <- ridge_model$a0 + X_test%*%ridge_model$beta
ridge_error <- sum((Y_test - ridge_predict)^2)
ridge_error


lasso_predict <- lasso_model$a0 + X_test%*%lasso_model$beta
Lasso_error<- sum((Y_test - lasso_predict)^2)
Lasso_error

#Records Holder
Error_record <- matrix(data = NA, nrow=4, ncol = 1)
rownames(Error_record) <- c("Linear Model", "AIC Linear Model", "Ridge Model", "Lasso Model")
colnames(Error_record) <- "Mean Squared Predicted Errors"
Error_record[1] <-linear_model_error
Error_record[2] <- AIC_model_error
Error_record[3] <- ridge_error
Error_record[4] <- Lasso_error

Error_record <- as.data.frame(Error_record)
Error_record
```

Lowest Average predicted Mean Square Value comes from our AIC model, which by AIC, gives the best predictors. 